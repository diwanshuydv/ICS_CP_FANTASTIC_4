# -*- coding: utf-8 -*-
"""rag_sample1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GLGRpXVd5MFuduyiGQ53tFveC_uLG6tI
"""

'''!pip install langchain
!pip install openai
!pip install pypdf
!pip install tiktoken
!pip install chromadb
!pip install streamlit'''

import langchain
import openai

openai.api_key = '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'
from langchain.document_loaders import PyPDFLoader

pages = PyPDFLoader(r"Constitution of Students Gymkhana_2_4_2018.pdf").load()

print(len(pages))
print(type(pages))

from langchain.text_splitter import RecursiveCharacterTextSplitter



chunk_size= 1500
chunk_overlap = 150

r_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
)

splits = r_splitter.split_documents(pages)

len(splits)

splits[0]

pages[0]

from langchain.embeddings.openai import OpenAIEmbeddings

embed = OpenAIEmbeddings(openai_api_key= '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')

#!pip install chromadb

from langchain.vectorstores import Chroma

#persist_directory = "/content/"

#!pip install tiktoken
import tiktoken
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embed,
)

print(vectordb._collection.count())

question="which iit are we talking about?"

ans = vectordb.similarity_search(question,k=3)

len(ans)

ans[0]

ans[1].page_content

ans[2]

vectordb.persist()

mmr_ans= vectordb.max_marginal_relevance_search(question, k=3
                                                )

mmr_ans[0]

from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(temperature = 0, model_name = 'gpt-3.5-turbo', openai_api_key= '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')

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever()
)

result = qa_chain({"query": question})

result

from langchain.prompts import PromptTemplate

# Build prompt
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

# Run chain
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)

result = qa_chain({"query": question})

result

qa_chain_mr = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    chain_type="map_reduce"
)
result = qa_chain_mr({"query": question})
result["result"]

llm.predict('printf("Hello world!"')

from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

from langchain.chains import ConversationalRetrievalChain
retriever=vectordb.as_retriever()
qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)

result = qa({"question": question})

result

result2 = qa({"question": "where is it located?"})

result2

qa({"question": "can we impeach student representative of our year?"})

#!pip install streamlit

import streamlit as st



# Streamlit UI
def main():
    st.title('Simple Chatbot')

    # Input field for the user to type their message
    user_message = st.text_input('Enter your message')

    # Button to submit the message
    if st.button('Send'):
        # Process the user input
        result = qa({"question": question})
        # Display the result
        st.write('Processed Result:', result)

if __name__ == '__main__':
    main()

!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py





